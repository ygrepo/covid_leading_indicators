import boto3
import botocore
import io
import pandas as pd
import pyarrow.parquet as pq

BUCKET_NAME = 'useast1-nlsn-w-digital-dsci-dev'
#FOLDER_NAME = 'users/gryv9001/20180201_20180203_view'
FOLDER_NAME = 'users/gryv9001/20180201_20180203_view_nocoviewing'
PATH='C:\\cygwin64\\home\\gryv9001\\code\\tv_dmp_viewers_breakdown\\data\\data_view.csv'


# Read single parquet file from S3
def pd_read_s3_parquet(key, bucket, s3_client=None, **args):
    if s3_client is None:
        s3_client = boto3.client('s3')
    obj = s3_client.get_object(Bucket=bucket, Key=key)
    return pd.read_parquet(io.BytesIO(obj['Body'].read()), **args)

# Read multiple parquets from a folder on S3 generated by spark
def pd_read_s3_multiple_parquets(filepath, bucket, s3=None,
                                 s3_client=None, verbose=False, **args):
    if not filepath.endswith('/'):
        filepath = filepath + '/'  # Add '/' to the end
    if s3_client is None:
        s3_client = boto3.client('s3')
    if s3 is None:
        s3 = boto3.resource('s3')
    s3_keys = [item.key for item in s3.Bucket(bucket).objects.filter(Prefix=filepath)
               if item.key.endswith('.parquet')]
    if not s3_keys:
        print('No parquet found in', bucket, filepath)
    elif verbose:
        print('Load parquets:')
        for p in s3_keys:
            print(p)
    dfs = [pd_read_s3_parquet(key, bucket=bucket, s3_client=s3_client, **args)
           for key in s3_keys]
    return pd.concat(dfs, ignore_index=True)

df = pd_read_s3_multiple_parquets(FOLDER_NAME, BUCKET_NAME)
data_count = len(df)
print('Loaded {} of data'.format(data_count))
df.to_csv(PATH)